import streamlit as st
import os
from langchain_google_genai import ChatGoogleGenerativeAI
from langgraph.graph import START, MessagesState, StateGraph
from langgraph.checkpoint.memory import MemorySaver
from langchain_core.messages import HumanMessage
import uuid

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(page_title="ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ", page_icon="ğŸ’¬")
st.title("ğŸ’¬ å¯¾è©±å‹ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ")

# LangGraphã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®åˆæœŸåŒ–
@st.cache_resource
def create_app():
    """LangGraphã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ä½œæˆ"""
    # Geminiãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
    api_key = os.environ.get("GEMINI_API_KEY")
    model = ChatGoogleGenerativeAI(
        model="gemini-2.0-flash",
        google_api_key=api_key,
        temperature=0.7
    )
    
    # ãƒ¢ãƒ‡ãƒ«ã‚’å‘¼ã³å‡ºã™é–¢æ•°
    def call_model(state: MessagesState):
        response = model.invoke(state["messages"])
        return {"messages": response}
    
    # ã‚°ãƒ©ãƒ•ã®å®šç¾©
    workflow = StateGraph(state_schema=MessagesState)
    workflow.add_edge(START, "model")
    workflow.add_node("model", call_model)
    
    # ãƒ¡ãƒ¢ãƒªã®è¿½åŠ 
    memory = MemorySaver()
    app = workflow.compile(checkpointer=memory)
    
    return app

# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å–å¾—
app = create_app()

# ã‚»ãƒƒã‚·ãƒ§ãƒ³åˆæœŸåŒ–ï¼ˆthread_idã§ä¼šè©±ã‚’ç®¡ç†ï¼‰
if "thread_id" not in st.session_state:
    st.session_state.thread_id = str(uuid.uuid4())

if "messages" not in st.session_state:
    st.session_state.messages = []

# éå»ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›
if prompt := st.chat_input("ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„"):
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤ºãƒ»ä¿å­˜
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®å¿œç­”
    with st.chat_message("assistant"):
        with st.spinner("è€ƒãˆä¸­..."):
            # LangGraphã®è¨­å®šï¼ˆthread_idã§ä¼šè©±ã‚’è­˜åˆ¥ï¼‰
            config = {"configurable": {"thread_id": st.session_state.thread_id}}
            
            # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’LangGraphå½¢å¼ã§é€ä¿¡
            input_message = HumanMessage(content=prompt)
            
            # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã§å¿œç­”ã‚’å–å¾—
            response_text = ""
            for event in app.stream({"messages": [input_message]}, config, stream_mode="values"):
                # æœ€å¾Œã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼ˆAIã®å¿œç­”ï¼‰ã‚’å–å¾—
                if event["messages"]:
                    last_message = event["messages"][-1]
                    if hasattr(last_message, 'content'):
                        response_text = last_message.content
            
            st.markdown(response_text)
    
    # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä¿å­˜
    st.session_state.messages.append({"role": "assistant", "content": response_text})

# ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«è¨­å®š
with st.sidebar:
    st.header("è¨­å®š")
    
    # ä¼šè©±å±¥æ­´ã‚’ã‚¯ãƒªã‚¢
    if st.button("æ–°ã—ã„ä¼šè©±ã‚’å§‹ã‚ã‚‹"):
        st.session_state.messages = []
        st.session_state.thread_id = str(uuid.uuid4())
        st.rerun()
    
    # ãƒ‡ãƒãƒƒã‚°æƒ…å ±
    with st.expander("ãƒ‡ãƒãƒƒã‚°æƒ…å ±"):
        st.write(f"Thread ID: {st.session_state.thread_id}")
        st.write(f"ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°: {len(st.session_state.messages)}")
        st.json(st.session_state.messages)